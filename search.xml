<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>第一章 Machine Learning基础</title>
      <link href="/2018/06/09/ml_chapter1/"/>
      <url>/2018/06/09/ml_chapter1/</url>
      <content type="html"><![CDATA[<p>哎，打脸了，第一篇就拖了一个星期，结果数据挖掘好像考的也不咋地，哭一会儿。<br><img src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1529322415583&di=c3f54b957d9f3161cc3adef03d59ba70&imgtype=0&src=http%3A%2F%2Fgfs12.gomein.net.cn%2FL1y9VgB7AT1RCvBVdK.gif" width="200" hegiht="100" align="center"><br>进入正题。</p><p>AI是近些年最火热的话题之一，而Machine Learning则在整个AI领域中占据着举足轻重的地位（尤其Deep Learning得到了飞跃发展的今天）。抛开大家在游戏（比如最近大红的底特律）又或影视中看到的非常概念性的AI产品（西部世界等），就近来看，Machine Learning在现在的科技产品中也扮演着越来越重要的角色，比如人脸识别，比如推荐系统等等。就我个人的想法，当这些特别的个体部分得到充分性的整合后，得到的必然将是颠覆性的产品概念（例如，已经步步走向成熟的智能驾驶系统）。<br>这一章，主要内容包括Machine Learning的定义，分类等大的概念，以及一些我对机器学习这方面自己的理解。（接下来统称ML）</p><h2 id="1-Definition-amp-Why"><a href="#1-Definition-amp-Why" class="headerlink" title="1. Definition &amp;Why"></a>1. Definition &amp;Why</h2><p>书中引用的是Tom Mitchell（1997）对ML的定义：</p><p><em>A computer program is said ot learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.</em></p><div align="center"><br><img src="/img/chapter1/ML_struc.png" width="400" hegiht="200" align="center"><br></div><p>一个简单的例子是，我们要做一个垃圾邮件的分类（T）模型，通过这个模型进行分类得到类别（是否为spam）P，我们通过一些已知分类的邮件输入到这个模型里，根据这些邮件调整这个模型的参数，使其的分类结果P的准确性在实际过程中得到提高。</p><p>当然，这里的数据的形式也是多种多样的，比如在Reinforcement Learning中则是通过定义rewards的方式，使机器在特定policy中，对自身行为进行评判并不断提升（这里的数据则为这个模型在这次表现的action以及其这套actions的结果）。<br>闲话聊多了， 那为什么要运用ML的方式设计模型呢？</p><p>书中的观点整理下来如下，</p><ul><li>Problems for which existing solutions require a lot of hand-tuning or long lists of  rules: one Machine Learning algorithm can often simplify code and perform better.（省事儿，不用设计人员自己进行参数调整）</li><li>Complex problems for which there is  no good solution at all using a traditional approach: the best Machine Learning techniques can find a solution.（光靠主观想，很难设计一个很好又周全的rule）</li><li>Fluctuating environments: a Machine Learning system can adapt to new data.Getting insights about complex problems and large amounts of data. （ML的适应性更强，并且能够get insight）</li></ul><p>总的这样看，ML的作用很明显了。接下来，我们先聊聊，ML这个大家族是怎么进行分类定义的。毕竟解决的问题是多种多样，数据方式也是多种多样的。那么，对应的模型、系统，自然也是。</p><h2 id="2-Types-of-Machine-Learning-Systems"><a href="#2-Types-of-Machine-Learning-Systems" class="headerlink" title="2. Types of Machine Learning Systems"></a>2. Types of Machine Learning Systems</h2><p>在书中，有3种分类的方法（当然，别的分类方式也是有的）：</p><ul><li><p>根据<strong>是否需要人类的supervision</strong>方式分类：<br>  – Supervised Learning<br>  – Unsupervised Learning<br>  – Semisupervised Learning<br>  – Reinforcement Learning</p></li><li><p><strong>训练数据的训练方式</strong>：<br>  – Online learning<br>  – Batch learning</p></li><li><p>根据<strong>模型原理</strong>，即其为简单的进行与已知数据比较还是通过对数据中的patterns学习到一个预测模型：<br>  – Instance-based learning<br>  – Model-based learning</p></li></ul><h3 id="2-1-根据人类的supervision"><a href="#2-1-根据人类的supervision" class="headerlink" title="2.1. 根据人类的supervision"></a>2.1. 根据人类的supervision</h3><h4 id="Supervised-Learning"><a href="#Supervised-Learning" class="headerlink" title="Supervised Learning"></a>Supervised Learning</h4><ul><li>In Supervised learning, the training data feed to algorithm includes the Label.</li><li>Task to solve：classification，numeric value(could also used to do the classification–”probability”).</li><li>Most important algorithms（<em>NIB</em>：表示后面的系列将不会总结这个算法）：<br>– K-nearest Neighbors<br>– Linear regression<br>– Logistic Regression<br>– Support Vector Machine(SVMs)<br>– Decision Trees and Random Forests<br>– Neural networks<div align="center"><br><img src="/img/chapter1/ins_spam.png" width="400" hegiht="200" align="center"><br></div></li></ul><p>图片里是一个简单的垃圾短信分类系统，这里的Label也就是-&gt;是ham还是spam，其他的例子比如图片分类，人脸识别等等。我个人的理解，这部分就很像你教小孩子，一样事物是什么的，然他的见识的多，自己摸索到套路了（模型的参数），也就会了，成功率也就高了。</p><h4 id="Unsupervised-learning"><a href="#Unsupervised-learning" class="headerlink" title="Unsupervised learning"></a>Unsupervised learning</h4><ul><li>In Unsupervised learning, the training data is Unlabeled.</li><li>Task to solve：Clustering，Visualizaiton and Dimensionality reduction, Association rule learning(NIB).</li><li>Most important algorithms：<ul><li><strong>Clustering</strong>:<br>  – K-Means(NIB)<br>  – Hierarchical cluster Analysis(HCA)(NIB)<br>  – Expectation Maximization(NIB)</li><li><strong>Visualizaiton and Dimensionality reduction</strong>:<br>  – Principal Component Analysis(PCA)<br>  – Kernel PCA<br>  – Locally-linear Embedding(LIE)(NIB)<br>  – T-distributed Stochastic Neighbor Embedding(t-SNE)(NIB)</li><li><strong>Association rule</strong>:<br>  – Apriori(NIB)<br>  – Eclat(NIB)<div align="center"><br><img src="/img/chapter1/unsupervised.png" width="400" hegiht="200" align="center"><br></div></li></ul></li></ul><p>这是一个clustering的例子，看到图第一眼想可能到的就是KNN等这种聚类算法。unsupervised learning广泛的被应用到，类似于用户分类，topic detection等等这类大数据提取topic又或clustering的方向（采取人为label的话显然是too expensive啦）。当然，既然它可以clustering，它当然就可以用于检测离群值（比如 anomaly detection）。减维的话有比较常见PCA和他的亲戚们，比较直接就是可以减到2～3维来提供viualization，缩减运算量当然也是重要的一点，当然也有很多功能和clustering是比较重合的。<br>Association rule则是寻找大数据中attributes间的联系（读的论文不多，我还没遇到过这个方向的实例）。大概的意思就是，比如，可以通过人们在超市里都买什么，找到买的东西的相应联系，然后下次就可以放一起方便用户购买了。<br>书中对于Unspervised的部分只重点提了PCA相关的算法（减维），个人的理解就是，整本书都是偏向介绍classification 和regression的所以，偏向数据挖掘的算法比如LDA，高斯混合模型等都没有提及。（差点忘了EM，吴老师都说过了，“上帝的算法”）所以这部分的坑就准备在DL前开一章的坑来讲。（日常画饼，哎）</p><h4 id="Semisupervised-learning"><a href="#Semisupervised-learning" class="headerlink" title="Semisupervised learning"></a>Semisupervised learning</h4><ul><li>In Semisupervised learning, the training data are partially labeled.</li><li>Task to solve：Google photos(upload all famliy photos, label several of them, and learner labels all of them)</li><li>Combinations of unsupervised learning and supervised algorithms.</li></ul><p>大部分Semisupervised learning都融合了前面的两种学习系统，书中举了一个比较明显的例子google photos，其实我个人的观点，这个例子不是很好，因为严格意义上来讲，Semisupervised learning是不包含人为干预的，这里则更倾向于active learning。</p><p><em>来点画外音</em> 按照wiki上来讲，active learning其实是Semi learning的一个小弟。在wiki上把他分到了semisupervised learning里了。我自己的观点稍有不同，主动学习的主要思想是：</p><ul><li>实际上，大部分的数据都是为未标记的，那么，从未标记数据中，给用户（又或expert）提一个query example来标记，使得用这个example训练后的收益最大化（甚至可以得到媲美和训练很多其他标记数据的结果）。（个人解释，非官方）</li></ul><p>从整体定义上来看，这个学习方式严格意义上来看，个人觉的不可以完全划分到Semisupervised learning的小弟行列，因为他是需要interaction的。<br>当然以上都是个人的观点，我自己感觉，定义这个东西吧，大概理解就ok。有这空，还是把钻牛角尖的时间，去推推公式更好点。</p><h4 id="Reinforcement-learning"><a href="#Reinforcement-learning" class="headerlink" title="Reinforcement learning"></a>Reinforcement learning</h4><p>In Reinforcement learning,  the system called an agent, can observe the environment, select and perform actions, and get rewards in return(or penalties in the form of negative rewards).</p><div align="center"><br><img src="/img/chapter1/reiforcement.png" width="400" hegiht="200" align="center"><br></div><p>这种方式很有趣，像我前面提到的，他通过制定相关的policy来给模型reward然后去学习，我个人的感觉，这是一个非常有潜力的学习类型，毕竟这才是自学习的精髓，重点是怎么来规定这个policy和reward，现在大部分的运用包括一些模型走路的学习训练（游戏里比较常见），又或者著名的AlphaGo。不出意外，可能这部分的详细整理会出现在Deep learning章的最后一部分，就当是压个轴吧（当然，不出意外的话，哭了）。</p><h3 id="2-2-根据输入数据是fed-by-batch还是online"><a href="#2-2-根据输入数据是fed-by-batch还是online" class="headerlink" title="2.2. 根据输入数据是fed by batch还是online"></a>2.2. 根据输入数据是fed by batch还是online</h3><h4 id="Batch-learning-offline-learning"><a href="#Batch-learning-offline-learning" class="headerlink" title="Batch learning(offline learning)"></a>Batch learning(offline learning)</h4><ul><li>In Batch learning, it must be trained using all the available data.</li></ul><p>这个例子很好说，比如ImageNet比赛中的各种CNN网络～一股脑的训练好去搞定接下来的问题吧。<br>这个方式的弊端也很容易理解，需要足够的数据，再者，再训练的成本比较高（updating）。</p><h4 id="Online-learning"><a href="#Online-learning" class="headerlink" title="Online learning"></a>Online learning</h4><ul><li>In Online learning, you train the system incrementally by feeding it data instances sequentially or small groups(mini-batch). The system can learn about new data on the fly.</li><li><em>Attentions</em> :<ul><li>Learning rate: how fast they should adapt to changing data.<br>  – High: learn fast quickly forget the old data, easily influenced by noise.<br>  – Low: learn slow, have more inertia(惯性), less sensitive to noise.</li><li>If bad data is fed to the system: the system’s performance will gradually decline.<br>  – To avoid: monitor your system closely and promptly switch learning off if detect a drop in performance.<div align="center"><br><img src="/img/chapter1/oline.png" width="400" hegiht="200" align="center"><br></div></li></ul></li></ul><p>这张图是一个简单的Online learning的结构，对比上面的batch learning就可以了，可以incrementally的学习。可以随用随学，很理想。但是缺点也很明显，如上<em>Attentions</em>。</p><h3 id="2-3-根据模型的工作方式-instance-model-based"><a href="#2-3-根据模型的工作方式-instance-model-based" class="headerlink" title="2.3. 根据模型的工作方式 instance/model based"></a>2.3. 根据模型的工作方式 instance/model based</h3><h4 id="Instance-based-learning"><a href="#Instance-based-learning" class="headerlink" title="Instance-based learning"></a>Instance-based learning</h4><ul><li>In  instance-based learning: the system learns the examples by heart, then generalizes to new cases using a similarity measure.<div align="center"><br><img src="/img/chapter1/Instance.png" width="400" hegiht="200" align="center"><br></div></li></ul><p>这部分，我的理解比较粗暴一点。分类的例子，把training data放到数据库，新的数据需要预测，直接和数据库中的例子做比较（这里的比较可以是欧式距离，马式距离等等），找到最优匹配，所以这个方法又称为“胜者为王”法。比较经典的算法，比如KNN。</p><h4 id="Model-based-learning"><a href="#Model-based-learning" class="headerlink" title="Model-based learning"></a>Model-based learning</h4><ul><li>In  Model-based learning: the generalize from a set of examples is to build a model of these examples, the use that model to make predictions.</li><li>Summary:<ul><li>Studied the data</li><li>Selected a model</li><li>Trained it on the training data(minimizing a cost function)</li><li>Apply the model for prediction<div align="center"><br><img src="/img/chapter1/Model.png" width="400" hegiht="200" align="center"><br></div></li></ul></li></ul><h2 id="3-ML中会遇得到种种问题"><a href="#3-ML中会遇得到种种问题" class="headerlink" title="3. ML中会遇得到种种问题"></a>3. ML中会遇得到种种问题</h2><h3 id="3-1-Insufficient-Quantity-of-Training-Data"><a href="#3-1-Insufficient-Quantity-of-Training-Data" class="headerlink" title="3.1. Insufficient Quantity of Training Data"></a>3.1. Insufficient Quantity of Training Data</h3><p>Machine Learning needs enough data for training, even though a simple problems the system needs typically thousands of examples.</p><h3 id="3-2-Non-representative-Training-Data"><a href="#3-2-Non-representative-Training-Data" class="headerlink" title="3.2. Non-representative Training Data"></a>3.2. Non-representative Training Data</h3><p>If the sample is too small, there will be more sampling noise.</p><h3 id="3-3-Poor-Quality-Training-Data"><a href="#3-3-Poor-Quality-Training-Data" class="headerlink" title="3.3. Poor-Quality Training Data"></a>3.3. Poor-Quality Training Data</h3><p>Samples are full of errors, outliers, and noise.</p><h3 id="3-4-Irrelevant-Features"><a href="#3-4-Irrelevant-Features" class="headerlink" title="3.4. Irrelevant Features"></a>3.4. Irrelevant Features</h3><p><em>Solution</em> :</p><ul><li>Features selection: selecting the most useful features to train on among existing features.</li><li>Feature extraction: combining existing features to produce a more useful one.</li><li>Creating new features by gathering new data.</li></ul><p>个人觉得，其中Feature extraction的思想很重要，无论是模型前的数据预处理，还是模型里的kernel去解决非线性问题（比如SVM）。</p><h3 id="3-5-Overfitting-training-data"><a href="#3-5-Overfitting-training-data" class="headerlink" title="3.5. Overfitting training data"></a>3.5. Overfitting training data</h3><p><em>Solution</em> :</p><ul><li>To simplify the model by selecting one with fewer parameters, by reducing the number of attributes in the training data or by constraining the model.</li><li>To gather more training data</li><li>To reduce the noise in the training data</li></ul><p>Overfitting这个问题很经典，也成为了之前Deep learning得到飞速发展前的一道深坎。简单的说就是，参数量太大了，导致训练中每个数据的特点都被模型用各个参数重点关注了（谁让咱手下多呢）。比如这张图，</p><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1207849-56df4ac9042948fd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" width="400" hegiht="200" align="center"><br></div><p>可以看出第二个模型就比较满足我们的需要（较为符合实际），虽然有1个点不是很理想。但是第三个图，显然就是用力过猛了（不太符合实际规律一般）。这里的给的解决方法比较笼统，具体在模型里可以有Regularization（例如l1，l2等），又或增加更大的随机性等等（后面根据具体算法具体来讲）。</p><h3 id="3-6-Underfitting-the-training-Data"><a href="#3-6-Underfitting-the-training-Data" class="headerlink" title="3.6. Underfitting the training Data"></a>3.6. Underfitting the training Data</h3><p><em>Solution</em> :</p><ul><li>Selecting a more powerful model.</li><li>Feeding better features to the learning algorithm.</li><li>Reducing the constraints on the model.</li></ul><p>这种方法说白了就是这个模型水平不行，搞不定这个case，去叫个大哥（more powerful models）或者给更多对应的工具（features）。<br>当然，根据上个问题也可能就是overfitting的问题修过度了。</p><h2 id="4-Testing-amp-Validating"><a href="#4-Testing-amp-Validating" class="headerlink" title="4. Testing &amp;Validating"></a>4. Testing &amp;Validating</h2><p>（T：测试这个模型实际中不中，V：调整一下模型里的固有Hyperparameters）</p><ul><li>3 parts:<ul><li>Training set: A set of examples for learning fit the parameters.</li><li>Validation set: A set of examples for tuning the hyperparameters.</li><li>Testing set: A set of examples only to access the performance</li></ul></li><li>Error rate: generalization error or out-of-sample error.</li><li>Training/testing set: usually 8:2<br><em>cross-validation</em> : training set is split into complementary subsets, and each model is trained against a different combination of these subsets and validated against the remaining parts.<div align="center"><br><img src="/img/chapter1/crossvalidation.png" width="400" hegiht="200" align="center"><br></div></li></ul><p>一般的model都经历这个training-validation-testing的过程（一般testing的数据是一开始就直接拿走的，为了保证没有bias，下一章会重点整理），训练好模型了， 但模型大部分都会有一些Hyperparameters，需要咱自己手动调整。这时就又需要一些数据来帮忙调整它们，也就是validation这一部分。最后用testing部分的数据来对这个模型做评价。<br>问题来了，利用一部分训练数据来单独做validation的话，显然就浪费了这部分的有用信息。这时就有了cross-validation这个概念。<br>如图，就是把training的部分分了几块，分别作为validation数据，然后平均一哈作为validation的结果。最后调整好Hyperparameters再总的来training。这个方法经常被用在例如gridsearchCV这种寻找好的Hyperparameters的方法里。</p><h2 id="5-唠叨一下"><a href="#5-唠叨一下" class="headerlink" title="5. 唠叨一下"></a>5. 唠叨一下</h2><p>到这里，这一章就结束了，这里分享一些我自己的一些小想法：<br>一个ML系统能否成功基本可以分为3个大部分，</p><ul><li>第一部分是数据，这个部分决定了模型最后能否真的投入应用，功能就算不强的模型，数据充足的话，它的实施性也是可以得到一定的保障的。（实施性）</li><li>第二部分是模型本身，这个部分相对的决定了这个系统的上限，足够的数据，更好的模型，就意味着更高的准确性（上限性）</li><li>最后的一部分则是硬件，Deep learning能够得到飞速发展的另一个原因也就是硬件上计算能力的进步，加上分布式计算的充分发展，都是系统得到实现的基础。（保证性）</li></ul><p>好啦，大概念的内容就这里截止了，下一章就是运用一个完整的分类项目来介绍一些机器学习中的基本技巧和概念。</p><p>画饼复习路艰难，越学越慢，那就慢慢走吧。<br><img src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1529322594639&di=e1f9a8795737706f0cffb9503d28fe71&imgtype=0&src=http%3A%2F%2Fe.hiphotos.baidu.com%2Fimage%2Fpic%2Fitem%2Ffaf2b2119313b07e86f96a4807d7912397dd8c2d.jpg" width="200" hegiht="100" align="center"></p>]]></content>
      
      <categories>
          
          <category> Machine_Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础概念 </tag>
            
            <tag> Machine Learning </tag>
            
            <tag> ML分类 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>关于blog的blablabla</title>
      <link href="/2018/06/04/myblog/"/>
      <url>/2018/06/04/myblog/</url>
      <content type="html"><![CDATA[<p>Hi， 这里是一个来自在CS道路上正在挣扎的码农狗的blog，在拖延症的影响下， （终于*3）用Github Pages把blog搭了。</p><p><strong>鉴于博主是个话唠，欢迎放松心态品尝。</strong></p><h2 id="关于blog"><a href="#关于blog" class="headerlink" title="关于blog"></a>关于blog</h2><h3 id="BLOG的目的"><a href="#BLOG的目的" class="headerlink" title="BLOG的目的"></a>BLOG的目的</h3><p>建这个blog最初目的，也是为了逼迫自己  <strong>周更</strong>  ，9月前复习和整理完之前的笔记。———&gt;&gt;主要还是为了治拖延症<br>如果你对Machine Learning，特别是Deep Learning感兴趣的，欢迎随时来跟我讨论。</p><h3 id="关于Machine-Learning"><a href="#关于Machine-Learning" class="headerlink" title="关于Machine Learning"></a>关于Machine Learning</h3><p>整个的更新的框架，我会直接按照这本书的顺序，一章一章的整理和总结。</p><p>   <strong>V V V</strong></p><p><img src="https://images-eu.ssl-images-amazon.com/images/I/51vPMQ3gJWL.jpg" alt="avatar"></p><p>当然，如果你是ML的初学，我也是非常推荐 <strong>Hands-On Machine Learning with Scikit-Learn &amp; TensorFlow</strong>这本书的，把茫茫的知识点，缕顺的“丝般顺滑”。</p><p>大致的分了两个部分：1.机器学习基础 2.深度学习</p><ul><li>由于笔记中懒惰的我选择了直接搬英文，当然也是怕自己的渣英文强行翻译的话，产生误解，所以主要的躯干我还是会选用英文继续写的.</li><li>除了书中的主要内容的总结，我还会写一点自己的理解和补充内容（如书中未提的EM算法啊，等等），同时也会结合一些自己之前读过的论文和项目做一些小讨论.</li><li>代码部分是基于python3，库的话主要就是sk-learn和tensorflow. </li></ul><p>接下来的第一篇文章呢，就是类似于机器学习的简介， 介绍一些机器学习的分类和基本概念。<br><strong>就算下周两门考试，我也要下周一前更，更更更！欢迎监督！！</strong></p><p>……..给自己画了个大饼，但愿别打脸吧,哎……</p><p><img src="http://ww1.rs.fanjian.net/c/9f/dd/4d/039f480b12dd61d8794d2d801c806c36.gif" alt="avatar"></p><p>最后，博主入ML的大坑没多久，所以欢迎大家纠正错误的部分，这也是我想写blog的最重要的部分，慎重的写自己的每一个总结，认真的听每一条指正。</p><h3 id="关于搭建blog"><a href="#关于搭建blog" class="headerlink" title="关于搭建blog"></a>关于搭建blog</h3><p>自己比较懒，也懒得折腾，就选用了比较方便的hexo搭建这个blog，主题则选用了material，如果有朋友在搭建blog上遇到了问题，或者配置文件上遇到问题也可以和我来进行讨论（当然如果我会的话“滑稽”）。<br>*手机端的评论暂不支持微信登陆（…)</p>]]></content>
      
      <categories>
          
          <category> Machine_Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> hexo </tag>
            
            <tag> 闲聊 </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
